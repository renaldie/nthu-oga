name: Check NTHU OGA Posts
on:
  schedule:
    - cron: '0 8 * * *'  # Runs at 8:00 UTC (adjust as needed)
  workflow_dispatch:  # Allows manual triggering

permissions:
  contents: write
  actions: write

jobs:
  check_changes:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v3
        with:
          fetch-depth: 0  # Fetch all history for proper git operations
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'  # Specify exact version
          
      - name: Debug environments and files
        run: |
          echo "GitHub workspace: $GITHUB_WORKSPACE"
          echo "Current directory: $(pwd)"
          echo "Directory contents: $(ls -la)"
          echo "Python version: $(python --version)"
          echo "Python executable: $(which python)"
          
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install requests beautifulsoup4
          pip list

      - name: Create data directory and test structure
        run: |
          mkdir -p data
          echo "Testing write access" > data/test.txt
          ls -la data/
          rm data/test.txt
        
      - name: Verify script existence and update if needed
        run: |
          mkdir -p scripts
          ls -la scripts/
          
          # Create or update the check_nthu.py script with the correct content
          cat > scripts/check_nthu.py << 'EOF'
import json
import os
import sys
import traceback
import requests
from bs4 import BeautifulSoup
from datetime import datetime

def send_notification(changes):
    """
    Send a single consolidated notification for all changes
    changes: list of new or updated items
    """
    if not changes:
        print("No changes to notify")
        return
        
    current_time = datetime.now().strftime("%Y-%m-%d %H:%M")
    body = [f"Updates in NTHU OGA Posts ({current_time})\n\n"]
    
    body.append("New updates in OGA Posts:\n")
    body.append("https://oga.site.nthu.edu.tw/p/403-1524-8945-1.php?Lang=en\n")
    
    for item in changes:
        body.append(f"{item['title']} - {item['url']}\n")
    
    # Print the notification content directly (will be captured for email)
    print(''.join(body))

def scrape_nthu_oga():
    """Scrape NTHU OGA website for posts"""
    print("Starting scraping NTHU OGA website...")
    
    URL = 'https://oga.site.nthu.edu.tw/p/403-1524-8945-1.php?Lang=en'
    
    try:
        # Add headers to make request more like a browser
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml',
            'Accept-Language': 'en-US,en;q=0.9',
        }
        
        print(f"Sending request to {URL}")
        resp = requests.get(URL, headers=headers, timeout=30)
        status_code = resp.status_code
        print(f"Received response with status code: {status_code}")
        
        if status_code != 200:
            print(f"Unexpected status code: {status_code}")
            print(f"Response content: {resp.text[:500]}...")
            resp.raise_for_status()
        
        content_type = resp.headers.get('Content-Type', '')
        print(f"Content type: {content_type}")
        
        # Print first 200 characters of response for debugging
        print(f"Response preview: {resp.text[:200]}...")
        
        soup = BeautifulSoup(resp.text, 'html.parser')
        
        # Debug: print all found div elements
        all_divs = soup.find_all('div')
        print(f"Found {len(all_divs)} div elements total")
        
        # Look for mtitle class divs
        mtitle_divs = soup.find_all('div', class_='mtitle')
        print(f"Found {len(mtitle_divs)} div elements with class 'mtitle'")
        
        if len(mtitle_divs) == 0:
            print("No mtitle divs found. Checking for alternative selectors...")
            # Try to find some common elements to understand the structure
            headlines = soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5'])
            print(f"Found {len(headlines)} headline elements")
            if headlines:
                print("Sample headlines:")
                for i, h in enumerate(headlines[:5]):
                    print(f"  {i+1}. {h.text.strip()}")
            
            links = soup.find_all('a')
            print(f"Found {len(links)} link elements")
            if links:
                print("Sample links:")
                for i, a in enumerate(links[:5]):
                    print(f"  {i+1}. Text: {a.text.strip()}, Href: {a.get('href', 'N/A')}")
        
        current_data = []
        for div in mtitle_divs:
            if div.a:
                title = div.a.text.strip()
                url = div.a.get('href', '')
                print(f"Found item: {title} - {url}")
                
                # Fix relative URLs
                if url and not url.startswith(('http://', 'https://')):
                    base_url = 'https://oga.site.nthu.edu.tw'
                    url = base_url + url if url.startswith('/') else base_url + '/' + url
                
                current_item = {
                    'title': title,
                    'url': url,
                    'scrape_date': datetime.now().strftime("%Y-%m-%d")
                }
                current_data.append(current_item)
        
        print(f"Successfully scraped {len(current_data)} items")
        
        # If no items found but page loaded, attempt a simpler approach
        if len(current_data) == 0 and status_code == 200:
            print("No items found with primary selector. Trying alternative approach...")
            # Try to find all links with news-like patterns
            news_links = []
            for a in soup.find_all('a'):
                href = a.get('href', '')
                text = a.text.strip()
                if text and href and len(text) > 10:  # Basic filter for likely news items
                    news_links.append({
                        'title': text,
                        'url': href if href.startswith(('http://', 'https://')) else f"https://oga.site.nthu.edu.tw{href if href.startswith('/') else '/'+href}",
                        'scrape_date': datetime.now().strftime("%Y-%m-%d")
                    })
            
            if news_links:
                print(f"Found {len(news_links)} items with alternative approach")
                current_data = news_links[:10]  # Limit to first 10 items
        
        if len(current_data) > 0:
            return current_data
        else:
            # Create a minimal fake data entry to avoid failure
            print("WARNING: No data could be scraped, creating fallback data")
            return [{
                'title': "Scraper Alert: No data found",
                'url': URL,
                'scrape_date': datetime.now().strftime("%Y-%m-%d")
            }]
        
    except requests.exceptions.RequestException as e:
        print(f"Error scraping website: {str(e)}")
        traceback.print_exc()
        # Return a minimal data item to prevent complete failure
        return [{
            'title': f"Scraper Error: {str(e)}",
            'url': URL,
            'scrape_date': datetime.now().strftime("%Y-%m-%d")
        }]
    except Exception as e:
        print(f"Unexpected error in scraper: {str(e)}")
        traceback.print_exc()
        # Return a minimal data item to prevent complete failure
        return [{
            'title': f"Scraper Unexpected Error: {str(e)}",
            'url': URL,
            'scrape_date': datetime.now().strftime("%Y-%m-%d")
        }]

def compare_data(current_data, previous_data):
    """Compare current and previous data and return list of changes"""
    if not previous_data:
        return current_data  # All items are new if no previous data

    updates = []
    for current_item in current_data:
        is_new = True
        for prev_item in previous_data:
            if current_item['title'] == prev_item['title'] and current_item['url'] == prev_item['url']:
                is_new = False
                break
        if is_new:
            updates.append(current_item)
    return updates

def ensure_data_directory():
    """Ensure the data directory exists"""
    data_dir = 'data'
    if not os.path.exists(data_dir):
        try:
            os.makedirs(data_dir)
            print(f"Created directory: {data_dir}")
        except Exception as e:
            print(f"Error creating directory: {str(e)}")
            raise

def save_json_safely(data, filename):
    """Safely save JSON data to file with error handling"""
    try:
        # First write to a temporary file
        temp_filename = filename + '.tmp'
        with open(temp_filename, 'w', encoding='utf-8') as f:
            json.dump(data, f, indent=2, ensure_ascii=False)
        
        # Then rename it to the target file (atomic operation)
        os.replace(temp_filename, filename)
        print(f"Successfully saved: {filename}")
    except Exception as e:
        print(f"Error saving file {filename}: {str(e)}")
        # Try one more time with force write
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(data, f, indent=2, ensure_ascii=False)
            print(f"Successfully saved with force write: {filename}")
        except Exception as e:
            print(f"Critical error saving file {filename}: {str(e)}")
            raise

def main():
    print("Starting main function...")
    print(f"Python version: {sys.version}")
    print(f"Working directory: {os.getcwd()}")
    
    try:
        # Debug environment variables
        print("Environment variables:")
        for key, value in os.environ.items():
            if key.startswith(('GITHUB_', 'CI')):  # Show only GitHub-related vars
                print(f"  {key}: {value}")
        
        ensure_data_directory()
        print("Data directory check completed")
        
        filename = 'data/nthu_oga_posts.json'
        
        current_data = scrape_nthu_oga()
        if not current_data:
            print("No data scraped, exiting with error")
            sys.exit(1)
        
        print(f"Successfully scraped {len(current_data)} items")
            
        previous_data = None
        if os.path.exists(filename):
            print(f"Found existing file: {filename}")
            try:
                with open(filename, 'r', encoding='utf-8') as f:
                    previous_data = json.load(f)
                print(f"Loaded previous data from {filename}")
            except Exception as e:
                print(f"Error reading previous data from {filename}: {str(e)}")
                print("Continuing with empty previous data")
        else:
            print(f"No existing file found at {filename}")
        
        updates = compare_data(current_data, previous_data)
        if updates:
            print(f"Found {len(updates)} updates")
            send_notification(updates)
            
            # Always save current data
            print(f"Attempting to save data")
            save_json_safely(current_data, filename)
            print(f"Saved data")
            
            # After sending notification, commit changes if not running in debug mode
            if 'CI' in os.environ or 'GITHUB_ACTIONS' in os.environ:
                print("Running in CI environment, attempting git operations")
                os.system('git config --global user.name "GitHub Action"')
                os.system('git config --global user.email "action@github.com"')
                os.system('git add data/*.json')
                commit_result = os.system('git commit -m "Update NTHU OGA posts data"')
                print(f"Git commit result: {commit_result}")
                push_result = os.system('git push')
                print(f"Git push result: {push_result}")
            else:
                print("Not running in CI environment, skipping git operations")
        else:
            print("No updates found")
            # Still save the current data to update the scrape date
            save_json_safely(current_data, filename)
        
        print("Script completed successfully")
                
    except Exception as e:
        print(f"Error in main function: {str(e)}")
        print("Full traceback:")
        traceback.print_exc()
        sys.exit(1)  # Exit with error code

if __name__ == '__main__':
    main()
EOF
          
          echo "Script has been created/updated successfully."
      
      - name: Run script with verbose output
        run: |
          # Run with extensive debug output
          python -u scripts/check_nthu.py
        # We want to see if the script fails, so don't continue on error
      
      - name: Capture and display script output
        if: success() || failure() # Run regardless of whether previous steps succeeded
        run: |
          # Create an output file if it doesn't exist
          touch output.txt
          
          # Check if script has generated any files
          echo "Checking data directory after script run:"
          ls -la data/
          
          # Display contents of data file if it exists
          if [ -f "data/nthu_oga_posts.json" ]; then
            echo "Contents of data file:"
            cat data/nthu_oga_posts.json
          fi
      
      - name: Check for notification trigger
        if: success() || failure() # Run regardless of whether previous steps succeeded
        run: |
          # Check for output file
          if [ -f "data/nthu_oga_posts.json" ]; then
            echo "Data file exists"
            
            # Check if the python script stdout contained a notification
            if grep -q "Updates in NTHU OGA Posts" output.txt 2>/dev/null; then
              echo "Notification content found in script output"
              
              # Extract notification content
              notification_content=$(grep -A 20 "Updates in NTHU OGA Posts" output.txt)
              
              echo "NOTIFICATION_CONTENT<<EOF" >> $GITHUB_ENV
              echo "$notification_content" >> $GITHUB_ENV
              echo "EOF" >> $GITHUB_ENV
              
              echo "CHANGES_DETECTED=true" >> $GITHUB_ENV
            else
              echo "Notification not found in script output. Using fallback approach..."
              
              # Extract the first item as notification content
              if command -v jq &> /dev/null; then
                echo "Using jq to parse JSON..."
                first_item=$(jq -r '.[0].title + " - " + .[0].url' data/nthu_oga_posts.json 2>/dev/null)
                scrape_date=$(jq -r '.[0].scrape_date' data/nthu_oga_posts.json 2>/dev/null)
                
                if [ -n "$first_item" ] && [ -n "$scrape_date" ]; then
                  echo "NOTIFICATION_CONTENT<<EOF" >> $GITHUB_ENV
                  echo "Updates in NTHU OGA Posts ($scrape_date)" >> $GITHUB_ENV
                  echo "" >> $GITHUB_ENV
                  echo "Most recent post: $first_item" >> $GITHUB_ENV
                  echo "More details at: https://oga.site.nthu.edu.tw/p/403-1524-8945-1.php?Lang=en" >> $GITHUB_ENV
                  echo "EOF" >> $GITHUB_ENV
                  
                  echo "CHANGES_DETECTED=true" >> $GITHUB_ENV
                else
                  echo "Could not extract item data with jq"
                fi
              else
                echo "jq not available, using basic JSON check"
                if grep -q "title" data/nthu_oga_posts.json; then
                  echo "NOTIFICATION_CONTENT<<EOF" >> $GITHUB_ENV
                  echo "Updates detected in NTHU OGA Posts" >> $GITHUB_ENV
                  echo "More details at: https://oga.site.nthu.edu.tw/p/403-1524-8945-1.php?Lang=en" >> $GITHUB_ENV
                  echo "EOF" >> $GITHUB_ENV
                  
                  echo "CHANGES_DETECTED=true" >> $GITHUB_ENV
                fi
              fi
            fi
          else
            echo "Data file not found"
          fi

      - name: Send email
        if: env.CHANGES_DETECTED == 'true'
        uses: dawidd6/action-send-mail@v4
        with:
          server_address: smtp.gmail.com
          server_port: 465
          username: ${{ secrets.EMAIL_USERNAME }}
          password: ${{ secrets.EMAIL_PASSWORD }}
          subject: Updates in NTHU OGA Posts
          to: ${{ secrets.EMAIL_USERNAME }}
          from: GitHub Action
          body: ${{ env.NOTIFICATION_CONTENT }}
        continue-on-error: true  # Don't fail if email sending fails

      - name: Create GitHub Issue if workflow fails
        if: failure()
        uses: actions/github-script@v6
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          script: |
            github.rest.issues.create({
              owner: context.repo.owner,
              repo: context.repo.repo,
              title: 'Workflow Failure: NTHU OGA Scraper',
              body: `The NTHU OGA scraper workflow failed on run #${context.runNumber}.\n\nPlease check the [workflow logs](https://github.com/${context.repo.owner}/${context.repo.repo}/actions/runs/${context.runId}) for more details.`
            })
